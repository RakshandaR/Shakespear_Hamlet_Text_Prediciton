{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9464c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\raksh/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data Collection \n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "# Load the dataset\n",
    "data = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "\n",
    "# Save to a text file\n",
    "with open('shakespeare_hamlet.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97128981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4818"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the text data\n",
    "with open('shakespeare_hamlet.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcc94f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input sequences and labels\n",
    "input_sequences = []\n",
    "for line in text.split('\\n'):                                   # the entire text is split into lines\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]        # each line is converted to a sequence of tokens\n",
    "    for i in range(1, len(token_list)):                         # for each token in the line (starting from the second token)   \n",
    "        n_gram_sequence = token_list[:i+1]                      # create n-gram sequences - sequences of increasing length :- if i = 1, then the list will be token_list[0:2] (first two word's tokens), i.e [3, 5] if i = 2, then the list will be token_list[0:3] (first three word's tokens), i.e [3, 5, 7] and so on\n",
    "        input_sequences.append(n_gram_sequence)                 # add the n-gram sequence to the list\n",
    "      # print(line, \":\", n_gram_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c180dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    1,  687],\n",
       "       [   0,    0,    0, ...,    1,  687,    4],\n",
       "       [   0,    0,    0, ...,  687,    4,   45],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    4,   45, 1047],\n",
       "       [   0,    0,    0, ...,   45, 1047,    4],\n",
       "       [   0,    0,    0, ..., 1047,    4,  193]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad sequences to ensure uniform input size\n",
    "max_sequence_len = max([len(x) for x in input_sequences])                                # find the maximum sequence length              \n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre') # add padding to the sequences so that all sequences have the same length - here maxlen = 14\n",
    "input_sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75433ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0    0    1]\n",
      " [   0    0    0 ...    0    1  687]\n",
      " [   0    0    0 ...    1  687    4]\n",
      " ...\n",
      " [   0    0    0 ...  687    4   45]\n",
      " [   0    0    0 ...    4   45 1047]\n",
      " [   0    0    0 ...   45 1047    4]]\n"
     ]
    }
   ],
   "source": [
    "# Create predictors and label\n",
    "X = input_sequences[:,:-1]  # all columns except the last column\n",
    "Y = input_sequences[:,-1]   # only the last column\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4332b3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 687    4   45 ... 1047    4  193]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dca3302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Since some words might be repeated, they would be converted to categories\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0709eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96881bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\raksh\\Miniconda3\\envs\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\raksh\\Miniconda3\\envs\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 13, 100)           481800    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 13, 150)           150600    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 13, 150)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4818)              486618    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1219418 (4.65 MB)\n",
      "Trainable params: 1219418 (4.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1)) # Embedding layer\n",
    "model.add(LSTM(150, return_sequences=True))                             # First LSTM layer\n",
    "model.add(Dropout(0.2))                                                 # Dropout layer to prevent overfitting\n",
    "model.add(LSTM(100))                                                    # Second LSTM layer\n",
    "model.add(Dense(total_words, activation='softmax'))                     # Output layer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55c3771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "644/644 [==============================] - 22s 35ms/step - loss: 5.4874 - accuracy: 0.0850 - val_loss: 7.1631 - val_accuracy: 0.0703\n",
      "Epoch 2/50\n",
      "644/644 [==============================] - 19s 30ms/step - loss: 5.3641 - accuracy: 0.0919 - val_loss: 7.2387 - val_accuracy: 0.0697\n",
      "Epoch 3/50\n",
      "644/644 [==============================] - 28s 43ms/step - loss: 5.2457 - accuracy: 0.0988 - val_loss: 7.3108 - val_accuracy: 0.0641\n",
      "Epoch 4/50\n",
      "644/644 [==============================] - 29s 45ms/step - loss: 5.1319 - accuracy: 0.1033 - val_loss: 7.4186 - val_accuracy: 0.0643\n",
      "Epoch 5/50\n",
      "644/644 [==============================] - 32s 50ms/step - loss: 5.0211 - accuracy: 0.1087 - val_loss: 7.4930 - val_accuracy: 0.0661\n",
      "Epoch 6/50\n",
      "644/644 [==============================] - 32s 49ms/step - loss: 4.9122 - accuracy: 0.1116 - val_loss: 7.6396 - val_accuracy: 0.0653\n",
      "Epoch 7/50\n",
      "644/644 [==============================] - 30s 46ms/step - loss: 4.8011 - accuracy: 0.1195 - val_loss: 7.7902 - val_accuracy: 0.0639\n",
      "Epoch 8/50\n",
      "644/644 [==============================] - 29s 45ms/step - loss: 4.7002 - accuracy: 0.1214 - val_loss: 7.9077 - val_accuracy: 0.0666\n",
      "Epoch 9/50\n",
      "644/644 [==============================] - 27s 42ms/step - loss: 4.6034 - accuracy: 0.1273 - val_loss: 8.0186 - val_accuracy: 0.0643\n",
      "Epoch 10/50\n",
      "644/644 [==============================] - 30s 46ms/step - loss: 4.4991 - accuracy: 0.1327 - val_loss: 8.1812 - val_accuracy: 0.0635\n",
      "Epoch 11/50\n",
      "644/644 [==============================] - 34s 53ms/step - loss: 4.4045 - accuracy: 0.1408 - val_loss: 8.2552 - val_accuracy: 0.0626\n",
      "Epoch 12/50\n",
      "644/644 [==============================] - 27s 42ms/step - loss: 4.3100 - accuracy: 0.1484 - val_loss: 8.4278 - val_accuracy: 0.0602\n",
      "Epoch 13/50\n",
      "644/644 [==============================] - 27s 42ms/step - loss: 4.2181 - accuracy: 0.1558 - val_loss: 8.5496 - val_accuracy: 0.0598\n",
      "Epoch 14/50\n",
      "644/644 [==============================] - 26s 40ms/step - loss: 4.1304 - accuracy: 0.1672 - val_loss: 8.6804 - val_accuracy: 0.0602\n",
      "Epoch 15/50\n",
      "644/644 [==============================] - 32s 49ms/step - loss: 4.0432 - accuracy: 0.1812 - val_loss: 8.7903 - val_accuracy: 0.0608\n",
      "Epoch 16/50\n",
      "644/644 [==============================] - 30s 46ms/step - loss: 3.9650 - accuracy: 0.1925 - val_loss: 8.9695 - val_accuracy: 0.0612\n",
      "Epoch 17/50\n",
      "644/644 [==============================] - 27s 43ms/step - loss: 3.8867 - accuracy: 0.2029 - val_loss: 9.0924 - val_accuracy: 0.0561\n",
      "Epoch 18/50\n",
      "644/644 [==============================] - 26s 41ms/step - loss: 3.8113 - accuracy: 0.2183 - val_loss: 9.2160 - val_accuracy: 0.0575\n",
      "Epoch 19/50\n",
      "644/644 [==============================] - 27s 42ms/step - loss: 3.7427 - accuracy: 0.2261 - val_loss: 9.3314 - val_accuracy: 0.0560\n",
      "Epoch 20/50\n",
      "644/644 [==============================] - 29s 45ms/step - loss: 3.6749 - accuracy: 0.2384 - val_loss: 9.4452 - val_accuracy: 0.0563\n",
      "Epoch 21/50\n",
      "644/644 [==============================] - 31s 48ms/step - loss: 3.6090 - accuracy: 0.2521 - val_loss: 9.5387 - val_accuracy: 0.0573\n",
      "Epoch 22/50\n",
      "644/644 [==============================] - 32s 50ms/step - loss: 3.5527 - accuracy: 0.2609 - val_loss: 9.6432 - val_accuracy: 0.0560\n",
      "Epoch 23/50\n",
      "644/644 [==============================] - 25s 39ms/step - loss: 3.4906 - accuracy: 0.2702 - val_loss: 9.7512 - val_accuracy: 0.0561\n",
      "Epoch 24/50\n",
      "644/644 [==============================] - 30s 47ms/step - loss: 3.4376 - accuracy: 0.2802 - val_loss: 9.8573 - val_accuracy: 0.0560\n",
      "Epoch 25/50\n",
      "644/644 [==============================] - 33s 51ms/step - loss: 3.3795 - accuracy: 0.2901 - val_loss: 9.9668 - val_accuracy: 0.0571\n",
      "Epoch 26/50\n",
      "644/644 [==============================] - 30s 46ms/step - loss: 3.3269 - accuracy: 0.2966 - val_loss: 10.0296 - val_accuracy: 0.0552\n",
      "Epoch 27/50\n",
      "644/644 [==============================] - 32s 49ms/step - loss: 3.2743 - accuracy: 0.3053 - val_loss: 10.1341 - val_accuracy: 0.0530\n",
      "Epoch 28/50\n",
      "644/644 [==============================] - 30s 47ms/step - loss: 3.2277 - accuracy: 0.3123 - val_loss: 10.2227 - val_accuracy: 0.0528\n",
      "Epoch 29/50\n",
      "644/644 [==============================] - 30s 47ms/step - loss: 3.1828 - accuracy: 0.3224 - val_loss: 10.2880 - val_accuracy: 0.0536\n",
      "Epoch 30/50\n",
      "644/644 [==============================] - 34s 52ms/step - loss: 3.1394 - accuracy: 0.3303 - val_loss: 10.4186 - val_accuracy: 0.0554\n",
      "Epoch 31/50\n",
      "644/644 [==============================] - 55s 85ms/step - loss: 3.0953 - accuracy: 0.3354 - val_loss: 10.4690 - val_accuracy: 0.0560\n",
      "Epoch 32/50\n",
      "644/644 [==============================] - 60s 94ms/step - loss: 3.0503 - accuracy: 0.3441 - val_loss: 10.5682 - val_accuracy: 0.0534\n",
      "Epoch 33/50\n",
      "644/644 [==============================] - 29s 44ms/step - loss: 3.0097 - accuracy: 0.3536 - val_loss: 10.6672 - val_accuracy: 0.0573\n",
      "Epoch 34/50\n",
      "644/644 [==============================] - 29s 45ms/step - loss: 2.9728 - accuracy: 0.3596 - val_loss: 10.7251 - val_accuracy: 0.0523\n",
      "Epoch 35/50\n",
      "644/644 [==============================] - 27s 42ms/step - loss: 2.9316 - accuracy: 0.3654 - val_loss: 10.7720 - val_accuracy: 0.0534\n",
      "Epoch 36/50\n",
      "644/644 [==============================] - 28s 44ms/step - loss: 2.8923 - accuracy: 0.3743 - val_loss: 10.8592 - val_accuracy: 0.0521\n",
      "Epoch 37/50\n",
      "644/644 [==============================] - 27s 42ms/step - loss: 2.8581 - accuracy: 0.3830 - val_loss: 10.9168 - val_accuracy: 0.0519\n",
      "Epoch 38/50\n",
      "644/644 [==============================] - 28s 44ms/step - loss: 2.8243 - accuracy: 0.3848 - val_loss: 10.9947 - val_accuracy: 0.0528\n",
      "Epoch 39/50\n",
      "644/644 [==============================] - 27s 43ms/step - loss: 2.7821 - accuracy: 0.3940 - val_loss: 11.0368 - val_accuracy: 0.0528\n",
      "Epoch 40/50\n",
      "644/644 [==============================] - 30s 47ms/step - loss: 2.7508 - accuracy: 0.4010 - val_loss: 11.1275 - val_accuracy: 0.0513\n",
      "Epoch 41/50\n",
      "644/644 [==============================] - 27s 42ms/step - loss: 2.7206 - accuracy: 0.4063 - val_loss: 11.1991 - val_accuracy: 0.0505\n",
      "Epoch 42/50\n",
      "644/644 [==============================] - 28s 43ms/step - loss: 2.6817 - accuracy: 0.4130 - val_loss: 11.2579 - val_accuracy: 0.0497\n",
      "Epoch 43/50\n",
      "644/644 [==============================] - 27s 42ms/step - loss: 2.6444 - accuracy: 0.4203 - val_loss: 11.3274 - val_accuracy: 0.0509\n",
      "Epoch 44/50\n",
      "644/644 [==============================] - 30s 46ms/step - loss: 2.6177 - accuracy: 0.4253 - val_loss: 11.4060 - val_accuracy: 0.0536\n",
      "Epoch 45/50\n",
      "644/644 [==============================] - 55s 85ms/step - loss: 2.5851 - accuracy: 0.4311 - val_loss: 11.4967 - val_accuracy: 0.0544\n",
      "Epoch 46/50\n",
      "644/644 [==============================] - 62s 96ms/step - loss: 2.5553 - accuracy: 0.4371 - val_loss: 11.5564 - val_accuracy: 0.0517\n",
      "Epoch 47/50\n",
      "644/644 [==============================] - 32s 49ms/step - loss: 2.5275 - accuracy: 0.4414 - val_loss: 11.5956 - val_accuracy: 0.0523\n",
      "Epoch 48/50\n",
      "644/644 [==============================] - 28s 44ms/step - loss: 2.4998 - accuracy: 0.4447 - val_loss: 11.6633 - val_accuracy: 0.0515\n",
      "Epoch 49/50\n",
      "644/644 [==============================] - 33s 52ms/step - loss: 2.4673 - accuracy: 0.4525 - val_loss: 11.7280 - val_accuracy: 0.0523\n",
      "Epoch 50/50\n",
      "644/644 [==============================] - 27s 42ms/step - loss: 2.4448 - accuracy: 0.4597 - val_loss: 11.7472 - val_accuracy: 0.0492\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "history = model.fit(X_train, y_train, epochs=50, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff54f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next words\n",
    "def predict_next_words(model, tokenizer, text, num_words):\n",
    "    for _ in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word_index = np.argmax(predicted, axis=1)[0]\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_word_index:\n",
    "                #text += \" \" + word\n",
    "                #break\n",
    "                return word\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30fbc7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: To be, or not to be\n",
      "Predicted Text: To be, or not to be buried in't and in denmarke where it waste is all a rat a rat\n"
     ]
    }
   ],
   "source": [
    "input_text = \"To be, or not to be\"\n",
    "print(f\"Input Text: {input_text}\")\n",
    "max_sequence_len = model.input_shape[1] + 1\n",
    "next_word = predict_next_words(model, tokenizer, input_text, max_sequence_len)\n",
    "print(f\"Predicted Text: {next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('lstm_text_generation_model.h5')\n",
    "\n",
    "# Save the tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
